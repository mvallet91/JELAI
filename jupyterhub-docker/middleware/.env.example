# Example environment variables for JELAI middleware
# Copy this file to .env and fill in your specific values.

# --- LLM Configuration ---

# Option 1: Ollama (Self-hosted, default if no WebUI key provided)
# URL of your Ollama server (running locally or remotely)
ollama_url=http://localhost:11434

# Option 2: OpenAI-compatible WebUI 
# If you provide a webui_api_key, the system will prefer using this WebUI.
# URL of your WebUI's OpenAI-compatible API endpoint
# webui_url=http://localhost:3000
# API Key for your WebUI (if required, leave blank or comment out if none)
# webui_api_key=your_webui_api_key_here

# --- Optional: Specify LLM Models ---
# If not set, defaults to 'gemma3:4b' in the code.
# Model used for background classification tasks (should be fast)
# ollama_classification_model=gemma3:4b
# Model used for generating the final pedagogical response
# ollama_response_model=gemma3:4b
